\section*{\label{sec:comparison}Comparison of parallelism techniques}

In summary, the choice between implicit parallelism, MPI, and OpenMP depends on %
the application, the architecture of the computing system, and the desired level %
of control over parallelization and communication. Each approach has its strengths %
and weaknesses, and the most suitable one will vary based on the specific requirements %
of the parallel computing task at hand.
Implicit parallelism is the simplest and most straightforward approach, but it %
offers the least control over the parallelization process. MPI and OpenMP, on the %
other hand, provide more granular control over parallelism and communication, %
but they require more effort and planning to implement effectively.
OpenMP is ideal for tasks that can be parallelized within a single node, such as %
loop-level parallelism, and it offers the simplest and most efficient solution %
for shared-memory systems. MPI, on the other hand, is better suited for larger-scale %
distributed computations that require communication across multiple nodes.

The hybrid model made of the synergy between MPI and OpenMP, capitalizes on the advantages of both paradigms. %
In this hybrid approach, MPI manages communication and coordination between different nodes or processors, %
while within each node, OpenMP guides the parallelization of tasks through multiple threads. This hybrid model %
is particularly advantageous in applications demanding both inter-node and intra-node parallelism, effectively %
leveraging the strengths of both MPI and OpenMP. \\
However, this approach is also the most complex and challenging to implement, requiring a thorough understanding %
of both MPI and OpenMP. Effective utilization of this hybrid model demands meticulous planning and thoughtful %
considerations regarding workload distribution, potential overheads, and the crucial aspect of load balancing. %
These factors are instrumental in guaranteeing optimal performance across this entire system.

